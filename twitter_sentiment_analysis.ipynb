{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guna-20/twitter/blob/main/twitter_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ7VThOnp9WX"
      },
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from google.colab import drive\n",
        "from tensorflow.keras.models import Sequential\n",
        "import pandas as pd\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2I9JjyFSs4LZ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oS2oJaDAtpAz",
        "outputId": "bf0c22af-8eae-4876-f206-9e96589a4f7c"
      },
      "source": [
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "3_C3RjKjtxh8",
        "outputId": "ca22ae2e-246b-4cdd-ac91-c5bdbbf8a0d0"
      },
      "source": [
        "cols=['sentiment','id','date','query','user','text']\n",
        "df=pd.read_csv(\"/content/drive/MyDrive/Projects/Twitter_sentiment_analysis/training.1600000.processed.noemoticon.csv\",\n",
        "               engine='python',\n",
        "               header=None,\n",
        "               names=cols,\n",
        "               encoding=\"Latin1\")\n",
        "#   Latin1 encoding because it accept any possible byte as input, convert into unicode character\n",
        "\n",
        "df.drop(['id','date','query','user'],axis=1,inplace=True)\n",
        "df.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1599995</th>\n",
              "      <td>4</td>\n",
              "      <td>Just woke up. Having no school is the best fee...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599996</th>\n",
              "      <td>4</td>\n",
              "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599997</th>\n",
              "      <td>4</td>\n",
              "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599998</th>\n",
              "      <td>4</td>\n",
              "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1599999</th>\n",
              "      <td>4</td>\n",
              "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         sentiment                                               text\n",
              "1599995          4  Just woke up. Having no school is the best fee...\n",
              "1599996          4  TheWDB.com - Very cool to hear old Walt interv...\n",
              "1599997          4  Are you ready for your MoJo Makeover? Ask me f...\n",
              "1599998          4  Happy 38th Birthday to my boo of alll time!!! ...\n",
              "1599999          4  happy #charitytuesday @theNSPCC @SparksCharity..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miTnPgROuPTW"
      },
      "source": [
        "def clean_data(tweet):\n",
        "  tweet = BeautifulSoup(tweet,\"lxml\").get_text()\n",
        "  #  \"<html><head><title>test<body><h1>page title</h3>\" ----> test page title\n",
        "  \n",
        "  tweet = re.sub(r\"@[a-zA-Z0-9]+\",'',tweet)\n",
        "  #  \"@hello i am good\" -----> \"i am good\"\n",
        "  \n",
        "  tweet = re.sub(r\"https?://[a-zA-Z0-9./]+\",'',tweet)\n",
        "  #  \"https://zindi.africa/competitions\" -----> \"\"\n",
        "  \n",
        "  tweet = re.sub(r\"[^a-zA-Z.!?']+\",\" \",tweet)\n",
        "  #   \"*hello i am good 900\" -----> hollo i am good\n",
        "  \n",
        "  tweet = re.sub(r\" +\",\" \",tweet)\n",
        "  #   double spaces are removed\n",
        "  return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2o-HoYcVVBy"
      },
      "source": [
        "cleaned_data=[clean_data(tweet) for tweet in df.text]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SAVfuBo22Q4G"
      },
      "source": [
        "df_labels=df.sentiment.values\n",
        "df_labels[df_labels==4]=1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeK-Ipiww7gN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0OyUzh5VDo2H",
        "outputId": "e8fb57e6-ccc1-4b7d-94a0-476afb328ad2"
      },
      "source": [
        "cleaned_data[0:2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\" Awww that's a bummer. You shoulda got David Carr of Third Day to do it. D\",\n",
              " \"is upset that he can't update his Facebook by texting it... and might cry as a result School today also. Blah!\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9kUBTtpp0FN"
      },
      "source": [
        "\n",
        "vocab_size = 2000000\n",
        "embedding_dim = 16\n",
        "max_length = 200\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fZAI_XobNiu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(cleaned_data, df_labels, test_size=0.30, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMTGSw9EJUu2"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "# num_words is no. of words it should take into account. and num_words should ve as much as possible (without stop words)\n",
        "\n",
        "tokenizer.fit_on_texts(cleaned_data)\n",
        "word_index = tokenizer.word_index\n",
        "#  \"The cat sat on the mat.\"  -----> {'cat': 2, 'mat': 5, 'on': 4, 'sat': 3, 'the': 1}\n",
        "# lesser the word_index value, it has occured frequently in the context\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "# values are assigned in place of words based on the word index\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "# trunc removing extra words\n",
        "# adding \"0\"s to sequence\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paRngJQckSop"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D51_vDq0kZ-7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFFaGCdX5_ZM"
      },
      "source": [
        "#We store this tokenizer in a file to use later in web app\n",
        "import pickle\n",
        "# saving\n",
        "with open('/content/drive/MyDrive/Projects/Twitter sentiment analysis/tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLxlrDyhTPB-"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPool1D, Dense, Dropout"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5j7TyFBbRK1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G95NswFGU86S"
      },
      "source": [
        "VOCAB_SIZE =vocab_size\n",
        "EMB_DIM = 200\n",
        "nb_filters  = 50  \n",
        "FFN_units = 200\n",
        "NB_CLASSES = 2\n",
        "dropout_rate = 0.2\n",
        "BATCH_SIZE = 32\n",
        "NB_EPOCHS = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlflSIbCTraw"
      },
      "source": [
        "model1 = Sequential()\n",
        "#  it does not allow you to create models that share layers or have multiple inputs or outputs\n",
        "\n",
        "model1.add(tf.keras.layers.Embedding( vocab_size, embedding_dim, input_length=max_length))\n",
        "# Word embeddings provide a dense representation of words and their relative meanings.\n",
        "# input_dim: This is the size of the vocabulary in the text data.\n",
        "# embeding_dim : a vector space of 16 dimensions\n",
        "# input_length : length of each input document \n",
        "\n",
        "model1.add(Conv1D(filters=nb_filters,\n",
        "                                kernel_size=2,\n",
        "                                padding = 'valid',\n",
        "                                activation = \"relu\"))\n",
        "# \"valid\" applies padding to the input sequence so the output size shrinks by filter_size - 1. No padding occurs.\n",
        "# relu because return positive values and negative values as zero.\n",
        "model1.add(Conv1D(filters = nb_filters,\n",
        "                                 kernel_size = 3,\n",
        "                                 padding = \"valid\",\n",
        "                                 activation = \"relu\"))\n",
        "model1.add(Conv1D(filters = nb_filters,\n",
        "                                 kernel_size = 4,\n",
        "                                 padding = \"valid\",\n",
        "                                activation = 'relu'))\n",
        "model1.add(GlobalMaxPool1D())\n",
        "# Downsamples the input representation by taking the maximum value over the time dimension.\n",
        "# shape = input_length,input_channel\n",
        "\n",
        "model1.add(Dense(units = FFN_units,activation = \"relu\"))\n",
        "model1.add(Dropout(rate = dropout_rate))\n",
        "# dropout layer is to prevent overfitting.\n",
        "\n",
        "model1.add(Dense(1, activation='sigmoid'))\n",
        "# \"sigmoid\" because we want output ranging between 0 and 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1t1RjwUnC3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61558793-cbb5-4098-ab1b-de5309b4d1f1"
      },
      "source": [
        "model1.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 200, 16)           32000000  \n",
            "_________________________________________________________________\n",
            "conv1d (Conv1D)              (None, 199, 50)           1650      \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 197, 50)           7550      \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 194, 50)           10050     \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d (Global (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 200)               10200     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 201       \n",
            "=================================================================\n",
            "Total params: 32,029,651\n",
            "Trainable params: 32,029,651\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBg8OMTfKW_t"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ytx6mFBfQM94"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IM9VAkWgRnQT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2Gv5y7kVcWK"
      },
      "source": [
        "model1.compile(loss = \"binary_crossentropy\",\n",
        "               optimizer = 'adam',\n",
        "               metrics = ['accuracy'])\n",
        "\n",
        "# binary_crossentropy because its a binary__classificartion problem (  -(1/N) sum(y * log(yhat) + (1 -y) * log(1 - yhat)) )\n",
        "# optimizer deals with learning rate and when should update weight."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KelrfM0TNjE"
      },
      "source": [
        "\n",
        "checkpoint_path = \"/content/drive/MyDrive/Projects/Twitter sentiment analysis/checkpoint/\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(model1)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=1)\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print(\"Latest checkpoint restored!!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "zY6cY3-9UQ6d",
        "outputId": "cdd420db-2b16-4840-e86c-28bc4e595d31"
      },
      "source": [
        "model1.fit(training_padded,y_train,validation_data= (testing_padded,y_test),batch_size = 2**8, epochs = NB_EPOCHS, verbose =1) \n",
        "ckpt_manager.save()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "4375/4375 [==============================] - 726s 166ms/step - loss: 0.4262 - accuracy: 0.8021 - val_loss: 0.3939 - val_accuracy: 0.8214\n",
            "Epoch 2/2\n",
            "4375/4375 [==============================] - 725s 166ms/step - loss: 0.3610 - accuracy: 0.8406 - val_loss: 0.3956 - val_accuracy: 0.8227\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Projects/Twitter sentiment analysis/checkpoint/ckpt-1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXahPtlnUs-y",
        "outputId": "f77fd901-e937-4f0e-951e-1f6ca6684ded"
      },
      "source": [
        "predict_1 = model1.predict(testing_padded,batch_size = 2**8,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1875/1875 [==============================] - 4s 2ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49g-zfUmU-YY"
      },
      "source": [
        "ori = lambda x:0 if x<0.5 else 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tuCNcrXVA4n"
      },
      "source": [
        "predict_1_y = list(map(ori,predict_1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3gY77zR_wMb",
        "outputId": "561ac04b-1c47-46a5-9a2a-b2cd378b0b41"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,predict_1_y))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.83      0.82    239361\n",
            "           1       0.83      0.81      0.82    240639\n",
            "\n",
            "    accuracy                           0.82    480000\n",
            "   macro avg       0.82      0.82      0.82    480000\n",
            "weighted avg       0.82      0.82      0.82    480000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-MbEe6FAMxs"
      },
      "source": [
        "sen = [\"i love him\"]\n",
        "tokenizer=tfds.deprecated.text.Tokenizer()\n",
        "input_data1 = [tokenizer.tokenize(i) for i in sen]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Nf9l4aCJ0B"
      },
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "sen = tokenizer.texts_to_sequences(input_data1)\n",
        "#sen= pad_sequences(sen, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n-bD8ifgkwu",
        "outputId": "33b8ab00-da10-4c12-9271-b655aec343e9"
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1120000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMEiogYki99N",
        "outputId": "fc55157b-a44c-4134-c36a-42410a6814c4"
      },
      "source": [
        "reviews = ['they made me cry', 'I hate spaghetti',\n",
        "           \"he couldn't make it\", \n",
        "                'Everything was good',\n",
        "                'he is a theif', \n",
        "                'Everything was green', \n",
        "                'the host seated us immediately',\n",
        "                'they gave us free chocolate cake', \n",
        "                'not sure about the wilted flowers on the table',\n",
        "                'only works when I stand on tippy toes', \n",
        "              \"everyone was happy \"]\n",
        "\n",
        "ori = lambda x:0 if x<0.5 else 1\n",
        "# Create the sequences\n",
        "padding_type='post'\n",
        "sample_data=[clean_data(tweet) for tweet in reviews]\n",
        "#sample_stop = [stop_remove(i) for i in sample_data]\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_data)\n",
        "reviews_padded = pad_sequences(sample_sequences, padding=padding_type, maxlen=max_length)           \n",
        "classes = model1.predict(reviews_padded)\n",
        "classes_y=list( map(ori,classes))\n",
        "for i in range(len(reviews)):\n",
        "  print(reviews[i],classes_y[i])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "they made me cry 0\n",
            "I hate spaghetti 0\n",
            "he couldn't make it 0\n",
            "Everything was good 1\n",
            "he is a theif 1\n",
            "Everything was green 1\n",
            "the host seated us immediately 1\n",
            "they gave us free chocolate cake 1\n",
            "not sure about the wilted flowers on the table 1\n",
            "only works when I stand on tippy toes 0\n",
            "everyone was happy  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYJmjWvj6Uw-"
      },
      "source": [
        "model1.save(\"/content/drive/MyDrive/Projects/Twitter sentiment analysis/sentient_model1.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poM3Y36G-1MU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HJhxTbZ-1Jb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU_cg8Fx-3Zh"
      },
      "source": [
        "**For web app**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NSuii0MVH1aO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc593123-1506-456c-c79b-938672647584"
      },
      "source": [
        "!pip install streamlit"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting streamlit\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b7/98/4725661dc5719c05ba7e3f9744407ce91e2d982cb6c9601de2bbb62e2dd0/streamlit-0.81.0-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 17.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.23.0)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.8.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Collecting gitpython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 45.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.12.4)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d6/bc/f0e44828e4290367c869591d50d3671a4d0ee94926da6cb734b7b200308c/pydeck-0.6.2-py2.py3-none-any.whl (4.2MB)\n",
            "\u001b[K     |████████████████████████████████| 4.2MB 36.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.19.5)\n",
            "Collecting watchdog; platform_system != \"Darwin\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/b2/b4ebe23174fd00ec94ac3f58ebf85f1090c49858feab1ca62ed7ea4d2f2f/watchdog-2.0.3-py3-none-manylinux2014_x86_64.whl (74kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.1.5)\n",
            "Collecting blinker\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1b/51/e2a9f3b757eb802f61dc1f2b09c8c99f6eb01cf06416c0671253536517b6/blinker-1.4.tar.gz (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 59.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit) (2.8.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (5.1.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit) (1.5.1)\n",
            "Collecting validators\n",
            "  Downloading https://files.pythonhosted.org/packages/db/2f/7fed3ee94ad665ad2c1de87f858f10a7785251ff75b4fd47987888d07ef1/validators-0.18.2-py3-none-any.whl\n",
            "Requirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.1.0)\n",
            "Requirement already satisfied: pyarrow; python_version < \"3.9\" in /usr/local/lib/python3.7/dist-packages (from streamlit) (3.0.0)\n",
            "Requirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit) (4.2.1)\n",
            "Collecting base58\n",
            "  Downloading https://files.pythonhosted.org/packages/b8/a1/d9f565e9910c09fd325dc638765e8843a19fa696275c16cc08cf3b0a3c25/base58-2.1.0-py3-none-any.whl\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.7/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from streamlit) (20.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->streamlit) (3.0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit) (56.0.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (7.6.3)\n",
            "Collecting ipykernel>=5.1.2; python_version >= \"3.4\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/7d/9f8ac1b1b76f2f1538b5650f0b5636bae082724b1e06939a3a9d38e1380e/ipykernel-5.5.3-py3-none-any.whl (120kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: jinja2>=2.10.1 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (2.11.3)\n",
            "Requirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit) (5.0.5)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.21.0->streamlit) (2018.9)\n",
            "Requirement already satisfied: decorator>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from validators->streamlit) (4.4.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (2.6.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.11.1)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit) (0.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->streamlit) (2.4.7)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.1.3)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (5.3.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.10.1->pydeck>=0.1.dev5->streamlit) (1.1.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.3.2->pydeck>=0.1.dev5->streamlit) (0.2.0)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.7.1)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.3.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.0.18)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (4.8.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.5)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.1)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (2.6.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=5.1.2; python_version >= \"3.4\"->pydeck>=0.1.dev5->streamlit) (22.0.3)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (5.6.1)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.5.0)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.9.4)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.2.5)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect; sys_platform != \"win32\"->ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.7.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.8.4)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (3.3.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (1.4.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.4.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit) (0.5.1)\n",
            "Building wheels for collected packages: blinker\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-cp37-none-any.whl size=13448 sha256=21e0b1ceb619e67766e1686ed5ce9464909042b0537fd60387b6098d61bdbc6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/a0/00/8690a57883956a301d91cf4ec999cc0b258b01e3f548f86e89\n",
            "Successfully built blinker\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement ipykernel~=4.10, but you'll have ipykernel 5.5.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: smmap, gitdb, gitpython, ipykernel, pydeck, watchdog, blinker, validators, base58, streamlit\n",
            "  Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "Successfully installed base58-2.1.0 blinker-1.4 gitdb-4.0.7 gitpython-3.1.14 ipykernel-5.5.3 pydeck-0.6.2 smmap-4.0.0 streamlit-0.81.0 validators-0.18.2 watchdog-2.0.3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "ipykernel"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hJ_kzc-k-1HI"
      },
      "source": [
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from bs4 import BeautifulSoup\n",
        "import streamlit as st"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dr4PoPko-1E9"
      },
      "source": [
        "def clean_data(tweet):\n",
        "  tweet = BeautifulSoup(tweet,\"lxml\").get_text()\n",
        "  tweet = re.sub(r\"@[a-zA-Z0-9]+\",'',tweet)\n",
        "  tweet = re.sub(r\"https?://[a-zA-Z0-9./]+\",'',tweet)\n",
        "  tweet = re.sub(r\"[^a-zA-Z.!?']+\",\" \",tweet)\n",
        "  tweet = re.sub(r\" +\",\" \",tweet)\n",
        "\n",
        "  return tweet"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-h6PGTo-1CZ"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bSuhkgqWX1"
      },
      "source": [
        "with open(\"/content/drive/MyDrive/Projects/Twitter_sentiment_analysis/tokenizer.pickle\", \"rb\") as handle:\n",
        " tokenizer = pickle.load(handle)\n",
        "model=load_model('/content/drive/MyDrive/Projects/Twitter_sentiment_analysis/sentient_model1.h5')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8NTHdJb_V6V",
        "outputId": "879188c1-ce83-4416-e785-cdd3d51af5a4"
      },
      "source": [
        "reviews = ['they made me cry', 'I hate spaghetti',\n",
        "           \"he couldn't make it\", \n",
        "                'Everything was good',\n",
        "                'he is a theif', \n",
        "                'Everything was green', \n",
        "                'the host seated us immediately',\n",
        "                'they gave us free chocolate cake', \n",
        "                'not sure about the wilted flowers on the table',\n",
        "                'only works when I stand on tippy toes', \n",
        "              \"everyone was happy \"]\n",
        "\n",
        "ori = lambda x:1 if x >0.6 else 0\n",
        "# Create the sequences\n",
        "padding_type='post'\n",
        "sample_data=[clean_data(tweet) for tweet in reviews]\n",
        "#sample_stop = [stop_remove(i) for i in sample_data]\n",
        "sample_sequences = tokenizer.texts_to_sequences(sample_data)\n",
        "reviews_padded = pad_sequences(sample_sequences, padding=padding_type, maxlen=200)           \n",
        "classes = model.predict(reviews_padded)\n",
        "classes_y=list(map(ori,classes))\n",
        "for i in range(len(reviews)):\n",
        "  print(reviews[i],classes_y[i])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 52) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 200).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-04 07:48:19.504 WARNING tensorflow: Model was constructed with shape (None, 52) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 200).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "they made me cry 0\n",
            "I hate spaghetti 0\n",
            "he couldn't make it 0\n",
            "Everything was good 1\n",
            "he is a theif 1\n",
            "Everything was green 1\n",
            "the host seated us immediately 1\n",
            "they gave us free chocolate cake 1\n",
            "not sure about the wilted flowers on the table 1\n",
            "only works when I stand on tippy toes 0\n",
            "everyone was happy  1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnMRD5r9AWt-"
      },
      "source": [
        "def predict(message):\n",
        " model=load_model('/content/drive/MyDrive/Projects/Twitter_sentiment_analysis/sentient_model1.h5')\n",
        " with open(\"/content/drive/MyDrive/Projects/Twitter_sentiment_analysis/tokenizer.pickle\", \"rb\") as handle:\n",
        "  tokenizer = pickle.load(handle)\n",
        " x_1 = tokenizer.texts_to_sequences([message])\n",
        " x_1 = pad_sequences(x_1, maxlen=500)\n",
        " predictions = model.predict(x_1)[0][0]\n",
        " ori = 1 if predictions >0.6 else 0\n",
        " return (predictions,ori)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORV28vUwAozC",
        "outputId": "23c92f19-8047-458e-a200-fa4317cfc33f"
      },
      "source": [
        "predict(\"i have to much of workload \")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Model was constructed with shape (None, 52) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 500).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2021-05-04 07:48:32.220 WARNING tensorflow: Model was constructed with shape (None, 52) for input KerasTensor(type_spec=TensorSpec(shape=(None, 52), dtype=tf.float32, name='embedding_input'), name='embedding_input', description=\"created by layer 'embedding_input'\"), but it was called on an input with incompatible shape (None, 500).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.22874278, 0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-2IGnucwWZHr",
        "outputId": "74c486ce-2166-48bb-9c17-c109e7d5781b"
      },
      "source": [
        "st.title(\"Movie Review Sentiment Analyzer\")\n",
        "message = st.text_area(\"Enter Review\",'Type Here ..')\n",
        "if st.button('Analyze'):\n",
        "  with st.spinner('Analyzing the text …'):\n",
        "    prediction=predict(message)\n",
        "    if prediction > 0.6:\n",
        "      st.success('Positive review with {:.2f} confidence'.format(prediction))\n",
        "      st.balloons()\n",
        "    elif prediction <0.4:\n",
        "      st.error('Negative review with {:.2f} confidence'.format(1-prediction))\n",
        "    else:\n",
        "      st.warning('Not sure! Try to add some more words')"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-04 08:10:32.171 WARNING root: \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py [ARGUMENTS]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "56Au8mpMbbmu",
        "outputId": "ff44e9ab-6a73-4871-9ced-2d8f0272311c"
      },
      "source": [
        "%%writefile app.py\n",
        "import pickle\n",
        "from tensorflow.keras.models import load_model\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import streamlit as st\n",
        "\n",
        "def predict(message):\n",
        " model=load_model('/content/drive/MyDrive/Projects/Twitter_sentiment_analysis/sentient_model1.h5')\n",
        " with open(\"/content/drive/MyDrive/Projects/Twitter_sentiment_analysis/tokenizer.pickle\", \"rb\") as handle:\n",
        "  tokenizer = pickle.load(handle)\n",
        " x_1 = tokenizer.texts_to_sequences([message])\n",
        " x_1 = pad_sequences(x_1, maxlen=500)\n",
        " predictions = model.predict(x_1)[0][0]\n",
        " ori = 1 if predictions >0.6 else 0\n",
        " return (predictions,ori)\n",
        "st.title(\"Movie Review Sentiment Analyzer\")\n",
        "message = st.text_area(\"Enter Review\",'Type Here ..')\n",
        "if st.button('Analyze'):\n",
        "  with st.spinner('Analyzing the text …'):\n",
        "    prediction=predict(message)\n",
        "    if prediction > 0.6:\n",
        "      st.success('Positive review with {:.2f} confidence'.format(prediction))\n",
        "      st.balloons()\n",
        "    elif prediction <0.4:\n",
        "      st.error('Negative review with {:.2f} confidence'.format(1-prediction))\n",
        "    else:\n",
        "      st.warning('Not sure! Try to add some more words')\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZeiLw21bhB5"
      },
      "source": [
        "run app.py "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-lP2Sc_uigW",
        "outputId": "b08d6aa7-836f-4871-d001-60fe08216169"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "pickle\n",
        "tensorflow\n",
        "streamlit"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYpGNqxUwPTx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}